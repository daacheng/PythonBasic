## 1.robots协议
也叫robots.txt，是存放在网站根目录下的文本文件，用来告诉搜索引擎该网站哪些内容是不应该被抓取的，哪些是可以抓取的。

如https://www.csdn.net/robots.txt
```python
User-agent: *
Disallow: /scripts
Disallow: /public
Disallow: /css/
Disallow: /images/
Disallow: /content/
Disallow: /ui/
Disallow: /js/
Disallow: /scripts/
Disallow: /article_preview.html*
Disallow: /tag/
Disallow: /*?*
Disallow: /link/

Sitemap: https://www.csdn.net/sitemap-aggpage-index.xml
Sitemap: https://www.csdn.net/article/sitemap.txt
```
## 2.常见的反爬虫措施
#### 1.请求头校验
一般网站会对请求头进行校验，比如Host，UA，Content-Type字段等，模拟请求的时候，这些常见的请求头最好是带上。
#### 2.IP访问次数控制
同一个IP地址短时间内大量发起请求，会引起IP限制，解决方法是用代理IP，或者构建自己的代理IP池。
#### 3.接口请求频率限制
有的网站会控制接口访问的频率，比如有些查询接口，控制两三秒访问一次。
#### 4.接口访问次数限制
每天限制某个IP或账号访问接口的次数，达到上限后出现二次验证或者直接封账号/IP.比如登录接口
#### 5.行为认证
请求次数过多会出现人工认证，如图片验证码，滑动认证，点击认证等，可以对接打码平台。
#### 6，自动化环境检测
selenium自动化工具有的网站会检测出来，大部分可以通过下面两种方式跳过检测,下面两种方式无法处理，还可以尝试把页面改为移动端页面(手机模式)，最后还有一种方法就是代理服务器拦截修改js代码，把检测selenium的js修改掉。
```python
options = webdriver.ChromeOptions()
# 躲避部分网站selenium检测
options.add_experimental_option('excludeSwitches', ['enable-automation'])
options.add_experimental_option("useAutomationExtension", False)

driver = webdriver.Chrome(executable_path=chromedriver_path, options=options)

# 躲避部分网站selenium检测
script = "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": script})
```

对于移动端appium的检测，可以尝试替换为uiautomator2实现自动化

#### 7.数据动态加载
有的数据不是通过html页面的接口请求返回的，抓包分析请求，找到正确的数据接口。
#### 8.请求参数加密
网易云音乐的post请求的请求体就是前端经过js加密后计算得到的，需要逆向js代码
#### 9.返回数据加密
需要逆向js代码，分析如何解密。还有一种像大众点评的评论，需要通过定位去找到文本。
#### 10.动态更新cookies
华为手机云服务，每次请求接口都会重新设置cookies，并且请求头参数也需要跟着cookies一起变化
